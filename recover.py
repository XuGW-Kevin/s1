import deepspeed
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint
import json
import os
import torch

# è·¯å¾„é…ç½®
base_dir = "ckpts/s1-20250423_064408"
checkpoint_tag = "global_step2500"
checkpoint_dir = os.path.join(base_dir, checkpoint_tag)
ds_config_path = "/workspace/s1/train/ds_config.json"

# åŠ è½½é…ç½®
with open(ds_config_path) as f:
    ds_config = json.load(f)

# åˆå§‹åŒ–æ¨¡å‹ç»“æ„
config = AutoConfig.from_pretrained(base_dir)
model = AutoModelForCausalLM.from_config(config)

# åˆå§‹åŒ– DeepSpeed Engine
model_engine, _, _, _ = deepspeed.initialize(
    model=model,
    config=ds_config,
    model_parameters=None
)

# åŠ è½½æƒé‡ï¼ˆåªæå– state_dictï¼‰
load_state_dict_from_zero_checkpoint(model_engine.module, checkpoint_dir)

# ğŸš€ ä¿å­˜ä¸º Hugging Face æ¨¡å‹æ ¼å¼
if torch.distributed.get_rank() == 0:
    output_dir = "./hf_export"
    os.makedirs(output_dir, exist_ok=True)

    model_engine.module.save_pretrained(output_dir)
    tokenizer = AutoTokenizer.from_pretrained(base_dir)
    tokenizer.save_pretrained(output_dir)

    print(f"âœ… å·²ä¿å­˜ä¸º Hugging Face æ ¼å¼æ¨¡å‹: {output_dir}/pytorch_model.bin")
